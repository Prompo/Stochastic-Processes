{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFOjD_VY0ld"
      },
      "source": [
        "<h1>Q-Learning</h1>\n",
        "\n",
        "<p>Στην συγκεκριμένη άσκηση θα μελετήσετε το στοχαστικό αλγόριθμο Q-Learning, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.</p> <p> Στα πλαίσια του παραδείγματος θα εξετάσετε μία υλοποίηση με Q-learning σχετικά με το σύστημα αυτόματης οδήγησης ενός ταξί. Στα πλαίσια του προβλήματος αυτού θα πρέπει να ικανοποιούνται τα εξής:</p>\n",
        "<ul>\n",
        "<li>Το ταξί θα πρέπει να αφήνει τον πελάτη στη σωστή θέση</li>\n",
        "<li>Το ταξί να ακολουθεί τη συντομότερη δυνατή διαδρομή</li>\n",
        "<li>Να τηρούνται οι κανόνες κυκλοφορίας και ασφάλειας των επιβατών</li>\n",
        "</ul>\n",
        "\n",
        "<p>Στα πλαίσια του προβλήματος έχουμε τα εξής χαρακτηριστικά για την επιβράβευση, τις καταστάσεις και τις ενέργειες.</p>\n",
        "\n",
        "<h4>Επιβράβευση</h4>\n",
        "<ul>\n",
        "<li>Θα έχουμε τη μέγιστη επιβράβευση όταν το ταξί αφήνει έναν πελάτη στην επιθυμητή θέση</li>\n",
        "<li>Θα υπάρχει ποινή στην περίπτωση όπου το ταξί αφήσει τον πελάτη σε κάποιο λανθασμένο σημείο</li>\n",
        "<li>Ο agent θα παίρνει μία μικρή σχετικά ποινή στην περίπτωση όπου αργεί να φτάσει στον τελικό προορισμό</li>\n",
        "</ul>\n",
        "\n",
        "<p>Γενικά οι παραπάνω αρχές συνοψίζονται στα εξής: \"Λαμβάνουμε +20 πόντους για μια επιτυχημένη πτώση και χάνουμε 1 πόντο για κάθε χρονικό βήμα που παίρνει. Υπάρχει επίσης ποινή 10 πόντων για παράνομες ενέργειες παραλαβής και αποχώρησης.\"</p>\n",
        "\n",
        "<h4>Πλήθος Καταστάσεων</h4>\n",
        "<img src=\"https://storage.googleapis.com/lds-media/images/Reinforcement_Learning_Taxi_Env.width-1200.png\">\n",
        "<p>Στο παράδειγμά μας έχουμε ένα μικρό χώρο 5*5. Από εκεί και πέρα έχουμε 4 προορισμούς και 5 πιθανές θέσεις του πελάτη (παίρνουμε και την περίπτωση ο πελάτης να είναι ήδη μέσα στο τάξι).</p>\n",
        "<p>Με βάση τα παραπάνω έχουμε 5*5*5*4=500 πιθανές καταστάσεις.</p>\n",
        "\n",
        "<h4>Πλήθος Ενεργειών</h4>\n",
        "<p>Έχουμε έξι ενεργειες για το ταξί, οι οποίες είναι οι εξής:</p>\n",
        "<ul>\n",
        "<li>0=Νότια</li>\n",
        "<li>1=Βόρεια</li>\n",
        "<li>2=Ανατολικά</li>\n",
        "<li>3=Δυτικά</li>\n",
        "<li>4=Επιβίβαση</li>\n",
        "<li>5=Αποβίβαση</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΣΧΟΛΙΟ-ΤΥΠΟΣ ΥΠΟΛΟΓΙΣΜΟΥ Q-learning:"
      ],
      "metadata": {
        "id": "upDooUocdpcU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ο αλγόριθμος Q-learning προσπαθεί να υπολογίσει την τιμή της βέλτιστης value function κατά Bellman μέσω της επαναληπτικής διαδικασίας"
      ],
      "metadata": {
        "id": "DAXARzKmdjzB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOQpQ0DygDzt"
      },
      "source": [
        "![1_hEl3nsWIsqi4fB51halCXw (1).webp](data:image/webp;base64,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)\n",
        "\n",
        "\n",
        "όπου α είναι ο ρυθμός εκμάθησης της διαδικασίας και γ είναι ο ρυθμός έκπτωσης (discount factor). Ο ρυθμός έκπτωσης επιτελεί την εξής λειτουργία: εάν ο πράκτορας λάβει κάποια επιβράβευση Rt+m στο βήμα t+m αντί για το βήμα t, τότε αυτή θα συνδέεται με την επιβράβευση, Rt+1 , εάν αυτή είχε ληφθεί στο βήμα t , μέσω της σχέσης Rt+m=γ^(m-1)Rt+1.\n",
        "\n",
        "Πριν συνεχίσετε στην άσκηση να απαντήσετε στο εξής ερώτημα:</p>\n",
        "\n",
        "<b><p>1. Να περιγράψετε σύντομα τον αλγόριθμο Q-Learning. Σε ποια προβλήματα θεωρείτε ότι ταιριάζει ως τρόπος εκμάθησης η Ενισχυτική Μάθηση (Reinforcement Learning); Ποια είναι η βασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration;</p></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΗ:"
      ],
      "metadata": {
        "id": "3MFsyM7FM11y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το Q-learning είναι ένας αλγόριθμος χωρίς μοντέλα, βασισμένος σε αξία, εκτός πολιτικής που θα βρει την καλύτερη σειρά ενεργειών με βάση την τρέχουσα κατάσταση του πράκτορα. Το \"Q\" σημαίνει ποιότητα. Η ποιότητα αντιπροσωπεύει πόσο πολύτιμη είναι η δράση για τη μεγιστοποίηση των μελλοντικών ανταμοιβών.  \n",
        "\n",
        "Οι αλγόριθμοι που βασίζονται σε μοντέλα χρησιμοποιούν συναρτήσεις μετάβασης και ανταμοιβής για να εκτιμήσουν τη βέλτιστη πολιτική και να δημιουργήσουν το μοντέλο. Αντίθετα, οι αλγόριθμοι χωρίς μοντέλα μαθαίνουν τις συνέπειες των πράξεών τους μέσω της εμπειρίας χωρίς συνάρτηση μετάβασης και ανταμοιβής.\n",
        "\n",
        "Η μέθοδος που βασίζεται στην αξία εκπαιδεύει τη συνάρτηση αξίας για να μάθει ποια κατάσταση είναι πιο πολύτιμη και να αναλάβει δράση. Από την άλλη πλευρά, οι μέθοδοι που βασίζονται σε πολιτικές εκπαιδεύουν την πολιτική απευθείας για να μάθει ποια δράση πρέπει να λάβει σε μια δεδομένη κατάσταση.\n",
        "\n",
        "Στο πλαίσιο εκτός πολιτικής , ο αλγόριθμος αξιολογεί και ενημερώνει μια πολιτική που διαφέρει από την πολιτική που χρησιμοποιείται για την εκτέλεση μιας ενέργειας. Αντίθετα, ο αλγόριθμος εντός πολιτικής αξιολογεί και βελτιώνει την ίδια πολιτική που χρησιμοποιείται για την εκτέλεση μιας ενέργειας.\n",
        "\n",
        "Βασικές ορολογίες:\n",
        "\n",
        "Κράτη(ες) : η τρέχουσα θέση του πράκτορα στο περιβάλλον.\n",
        "\n",
        "Δράση(α) : ένα βήμα που γίνεται από τον πράκτορα σε μια συγκεκριμένη κατάσταση.\n",
        "\n",
        "Ανταμοιβές : για κάθε ενέργεια, ο πράκτορας λαμβάνει ανταμοιβή και ποινή.\n",
        "\n",
        "Επεισόδια : το τέλος της σκηνής, όπου οι πράκτορες δεν μπορούν να αναλάβουν νέα δράση. Συμβαίνει όταν ο πράκτορας έχει πετύχει τον στόχο ή έχει αποτύχει.\n",
        "\n",
        "Q(S t+1 , a) : αναμενόμενη βέλτιστη τιμή Q για την εκτέλεση της ενέργειας σε μια συγκεκριμένη κατάσταση.\n",
        "\n",
        "Q(S t , A t ) : είναι η τρέχουσα εκτίμηση του Q(S t+1 , a).\n",
        "\n",
        "Q-Table : ο πράκτορας διατηρεί τον Q-πίνακα συνόλων καταστάσεων και ενεργειών.\n",
        "\n",
        "Χρονικές διαφορές (TD) : χρησιμοποιείται για την εκτίμηση της αναμενόμενης τιμής του Q(S t+1 , a) χρησιμοποιώντας την τρέχουσα κατάσταση και ενέργεια και την προηγούμενη κατάσταση και ενέργεια.\n",
        "\n",
        "\n",
        "Ο αλγόριθμος Q-learning προσφέρει  έξυπνες λύσεις. Πρέπει να ενημερώσουμε την τιμή των ζευγών κατάστασης-ενέργειας (κάθε κελί στον πίνακα Q) λαμβάνοντας υπόψη 1/ την ανταμοιβή για την επίτευξη της επόμενης κατάστασης και 2/ την υψηλότερη δυνατή τιμή στην επόμενη κατάσταση. Πάντα προσπαθούμε να βρούμε το καλύτερο δυνατό συνδυασμό.\n",
        "\n",
        "Reinforcement Learning - RL:\n",
        "\n",
        "Είναι ένα υποπεδίο της μηχανικής μάθησης που ασχολείται με τον τρόπο μάθησης των αλγορίθμων για να προσαρμόζονται και να λαμβάνουν αποφάσεις σε ένα περιβάλλον με βάση την αλληλεπίδραση τους με αυτό το περιβάλλον. Ο στόχος του reinforcement learning είναι η εκμάθηση μιας πολιτικής (policy) η οποία είναι ένας κανόνας ή μια στρατηγική , που καθορίζει ποιες ενέργειες πρέπει να πραγματοποιήσει ένας αλγόριθμος σε κάθε κατάσταση του περιβάλλοντος, με σκοπό να μεγιστοποιήσει την ανταμοιβή.Αυτή η μέθοδος μάθησης είναι κατάλληλη όταν έχουμε έναν ανεπτυγμένο περιβάλλον, στο οποίο ένας αλγόριθμος μπορεί να λάβει δράσεις και να προσαρμόσει τη συμπεριφορά του μέσω των ανταμοιβών ή κυρώσεων που λαμβάνει ανάλογα με τις ενέργειές του.\n",
        "\n",
        "Ορισμένες εφαρμογές της ενισχυτικής μάθηση είναι:\n",
        "\n",
        "Παιχνίδια: Η RL έχει εφαρμοστεί ευρέως σε παιχνίδια όπως το σκάκι, το Go, τα βιντεοπαιχνίδια, το poker κλπ., όπου ο αλγόριθμος μπορεί να μάθει να προσαρμόσει τη στρατηγική του για να επιτύχει τον αριθμό των νικών.\n",
        "\n",
        "Ρομποτική: Σε προβλήματα ελέγχου ρομπότ, όπως η πλοήγηση, η αντιστάθμιση, η αναγνώριση αντικειμένων κλπ., η RL μπορεί να βοηθήσει τα ρομπότ να μάθουν πώς να εκτελούν καλύτερα τις εργασίες τους μέσω δοκιμών και σφαλμάτων.\n",
        "\n",
        "Διαχείριση Ενέργειας: Σε εφαρμογές όπως η διαχείριση ενέργειας σε δίκτυα ενέργειας, η RL μπορεί να βοηθήσει στον αυτοματισμό της λειτουργίας και τη βελτιστοποίηση της χρήσης της ενέργειας.\n",
        "\n",
        "Συστήματα Παραγωγής: Σε εφαρμογές παραγωγής, όπως ο έλεγχος των πόρων σε βιομηχανικά περιβάλλοντα ή ο βελτιστοποίηση της παραγωγής σε αγροτικά συστήματα, η RL μπορεί να βοηθήσει στη λήψη αποφάσεων που οδηγούν σε βελτίωση της απόδοσης και της αποδοτικότητας.\n",
        "\n",
        "Διαχείριση Πόρων: Σε προβλήματα διαχείρισης πόρων, όπως η δρομολόγηση και η εκτέλεση εργασιών σε δίκτυα τηλεπικοινωνιών, η RL μπορεί να βοηθήσει στην εύρεση βέλτιστων λύσεων με βάση τις αλλαγές στο περιβάλλον και τις ανάγκες του συστήματος.\n",
        "\n",
        "\n",
        "Bασική διαφορά του αλγορίθμου Q-Learning από τους αλγορίθμους Policy Iteration και Value Iteration:\n",
        "\n",
        "Η κύρια διαφορά του αλγόριθμου Q-learning με τους αλγορίθμους Policy Iteration και Value Iteration είναι ότι ο αλγόριθμος Q-learning δεν πραγματοποιεί υποθέσεις για το περιβάλλον του υπό μελέτη συστήματος. Είναι, δηλαδή, ένας model-independent αλγόριθμος, γεγονός το οποίο συμβάλλει στην καθολικότητά του (δηλαδή τη χρήση του σε πληθώρα προβλημάτων) και κυρίως στη μείωση της χρονικής του πολυπλοκότητας. Η μείωση αυτή συμβαίνει διότι ο αλγόριθμος Q-learning δεν ασχολείται με την αρχικοποίηση ή την βελτιστοποίηση κάποιας πολιτικής.\n",
        "\n",
        "Στον αλγόριθμο Q-learning, ο πράκτορας δεν γνωρίζει πιθανότητες μετάβασης κατάστασης ή ανταμοιβές . Ο πράκτορας ανακαλύπτει ότι υπάρχει μια ανταμοιβή για τη μετάβαση από μια κατάσταση στην άλλη μέσω μιας δεδομένης ενέργειας όταν το κάνει και λαμβάνει μια ανταμοιβή. Ομοίως, υπολογίζει μόνο ποιες μεταβάσεις είναι διαθέσιμες από μια δεδομένη κατάσταση καταλήγοντας σε αυτήν την κατάσταση και εξετάζοντας τις επιλογές της. Εάν οι μεταβάσεις καταστάσεων είναι στοχαστικές, μαθαίνει την πιθανότητα μετάβασης μεταξύ καταστάσεων παρατηρώντας πόσο συχνά συμβαίνουν διαφορετικές μεταβάσεις.\n",
        "\n"
      ],
      "metadata": {
        "id": "dCH5KpI4M36Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C7fn6u31dXlO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc11bCDZgfSx"
      },
      "source": [
        "<p>Στη συνέχεια θα πρέπει να φορτώσετε τη βιβλιοθήκη gym καθώς και το σχετικό dataset<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDHLb5PGWdwr",
        "outputId": "dde72bfd-a4f4-424f-a38e-2cb36a2ab366",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
        "# Reset the environment to start\n",
        "env.reset()\n",
        "\n",
        "# Render the initial state\n",
        "env.render()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[110, 109, 106],\n",
              "         [110, 109, 106],\n",
              "         [124, 122, 122],\n",
              "         ...,\n",
              "         [108, 111, 109],\n",
              "         [108, 111, 109],\n",
              "         [118, 119, 119]],\n",
              " \n",
              "        [[110, 109, 106],\n",
              "         [110, 109, 106],\n",
              "         [124, 122, 122],\n",
              "         ...,\n",
              "         [108, 111, 109],\n",
              "         [108, 111, 109],\n",
              "         [118, 119, 119]],\n",
              " \n",
              "        [[114, 116, 115],\n",
              "         [114, 116, 115],\n",
              "         [126, 127, 126],\n",
              "         ...,\n",
              "         [112, 113, 111],\n",
              "         [112, 113, 111],\n",
              "         [118, 117, 115]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[116, 115, 116],\n",
              "         [116, 115, 116],\n",
              "         [106, 107, 108],\n",
              "         ...,\n",
              "         [113, 115, 114],\n",
              "         [113, 115, 114],\n",
              "         [117, 114, 117]],\n",
              " \n",
              "        [[116, 115, 116],\n",
              "         [116, 115, 116],\n",
              "         [106, 107, 108],\n",
              "         ...,\n",
              "         [113, 115, 114],\n",
              "         [113, 115, 114],\n",
              "         [117, 114, 117]],\n",
              " \n",
              "        [[115, 112, 112],\n",
              "         [115, 112, 112],\n",
              "         [119, 119, 117],\n",
              "         ...,\n",
              "         [123, 119, 118],\n",
              "         [123, 119, 118],\n",
              "         [114, 114, 117]]], dtype=uint8)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "396kTtOrW-cO",
        "outputId": "51d5440f-9880-467a-b6a6-3fa6ca761e83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.reset() # reset environment to a new, random state\n",
        "\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "print(\"State Space {}\".format(env.observation_space))\n",
        "\n",
        "# Render the initial state\n",
        "env.render()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[[110, 109, 106],\n",
              "         [110, 109, 106],\n",
              "         [124, 122, 122],\n",
              "         ...,\n",
              "         [108, 111, 109],\n",
              "         [108, 111, 109],\n",
              "         [118, 119, 119]],\n",
              " \n",
              "        [[110, 109, 106],\n",
              "         [110, 109, 106],\n",
              "         [124, 122, 122],\n",
              "         ...,\n",
              "         [108, 111, 109],\n",
              "         [108, 111, 109],\n",
              "         [118, 119, 119]],\n",
              " \n",
              "        [[114, 116, 115],\n",
              "         [114, 116, 115],\n",
              "         [126, 127, 126],\n",
              "         ...,\n",
              "         [112, 113, 111],\n",
              "         [112, 113, 111],\n",
              "         [118, 117, 115]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[116, 115, 116],\n",
              "         [116, 115, 116],\n",
              "         [106, 107, 108],\n",
              "         ...,\n",
              "         [113, 115, 114],\n",
              "         [113, 115, 114],\n",
              "         [117, 114, 117]],\n",
              " \n",
              "        [[116, 115, 116],\n",
              "         [116, 115, 116],\n",
              "         [106, 107, 108],\n",
              "         ...,\n",
              "         [113, 115, 114],\n",
              "         [113, 115, 114],\n",
              "         [117, 114, 117]],\n",
              " \n",
              "        [[115, 112, 112],\n",
              "         [115, 112, 112],\n",
              "         [119, 119, 117],\n",
              "         ...,\n",
              "         [123, 119, 118],\n",
              "         [123, 119, 118],\n",
              "         [114, 114, 117]]], dtype=uint8)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UC6-XuIhF5_"
      },
      "source": [
        "<p>Παρακάτω ορίζουμε τις συνεταγμένες του ταξί, τη θέση του πελάτη και το σημείο προορισμού</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPSOw5CdXFx1",
        "outputId": "5c4e7655-2e6f-4447-d6d6-c083b9fcfeb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsAkQJhchVFy"
      },
      "source": [
        "<p>Παρακάτω είναι η μήτρα επιβράβευσης για το state που ορίσαμε στο προηγούμενο βήμα</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7oDbznIXOJo",
        "outputId": "1e7d0708-e5b7-4485-9ed7-26f449497e2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.P[328]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtROeD1ph6kk"
      },
      "source": [
        "<p> Τρέχουμε το παράδειγμά μας χωρις τη χρήση Q-Learning.</p>\n",
        "\n",
        "<b><p>2. Τα αποτελέσματα είναι ικανοποιητικά; Πως θα μας εξυπηρετούσε η χρήση του Q-Learning;</p></b>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKHhcDxVXUFz",
        "outputId": "7dd59d76-3967-4707-ebfd-695d5cdd9c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "env.s = 328  # set environment to illustration's state\n",
        "\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "    # Put each rendered frame into dict for animation\n",
        "    frames.append({\n",
        "        'frame': env.render(),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "\n",
        "    epochs += 1\n",
        "\n",
        "\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))\n",
        "print(\"\\n\\n\")\n",
        "#print(f\"Results after {episodes} episodes:\")\n",
        "#print(f\"Average timesteps per episode: {total_epochs / episodes:.2f}\")\n",
        "#print(f\"Average penalties per episode: {total_penalties / episodes:.2f}\")\n",
        "#rewards = np.asarray(rewards)\n",
        "#print(f\"Average reward per step: {rewards.mean():.2f}\")\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 200\n",
            "Penalties incurred: 60\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΗ:"
      ],
      "metadata": {
        "id": "e3nI4yOcy1vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Από το αποτέλεσμα του παραπάνω προγράμματος μπορούμε να βγάλουμε κάποια χρήσιμα συμπέρασματα. Αρχικά, βλέπουμε ότι πραγματοποιήθηκαν 1347 βήματα. Ωστόσο, στα 425 από αυτά συνέβη κάποιο σφάλμα. Το ποσοστό σφάλματος, λοιπόν ανέρχεται στο 32%. Συνεπώς, μιλάμε για ένα μη αμελητέο ποσοστό το οποίο οφείλει να μας προβληματίσει.\n",
        "\n",
        "Συνεπώς, τα αποτελέσματα είναι μη ικανοποιητικά. Μία πιθανή εξήγηση που μπορούμε να δώσουμε, είναι πως το παραπάνω πρόγραμμα δεν ακολουθεί κάποιον αλγόριθμο, ώστε να μπορεί να μεγιστοποιήσει την ανταμοιβή και να ελαχιστοποιήση το σφάλμα.\n",
        "\n",
        "Άρα, για να βελτιώσουμε τα αποτελέσματα, πρέπει να εφαρμόσουμε κάποιον αλγόριθμο βελτιστοποίησης.Ο αλγόριθμος Q-learning φαντάζει ιδανική περίπτωση, διότι μπορεί να βοηθήσει τον agent να επιλέξει το βέλτιστο δυνατό μονοπάτι και να μεγιστοποιήσει την ανταμοιβή του. Επίσης, ο αλγόριθμος Q-learning εκτελείται ταχύτερα από τους άλλους 2 αλγορίθμους βελτιστοποίησης (Policy Iteration - Value Iteration).\n",
        "\n",
        "Υ.Γ.\n",
        "\n",
        "Το απότελεσμα που απεικονίζεται είναι διαφορετικό από αυτό που δόθηκε στην αρχή της άσκησης. Αυτό συνέβη διότι εκάνα κάποιους πειρματισμούς. Παρ' όλα αυτά και ανεξαρτήτως αποτελέσματος, η απάντηση μου και η αιτιολόγηση της άσκησης παραμένει ίδια."
      ],
      "metadata": {
        "id": "hXLFRb87y5OE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj3s09rsizVm"
      },
      "source": [
        "<p>Τώρα θα προσπαθήσουμε να λύσουμε το πρόβλημά μας με τη χρήση του Q-Learning.</p>\n",
        "\n",
        "<b><p>3. Τι γνωρίζετε για τις παραμέτρους α και γ. Τι θα συμβεί αν έχουν τιμές ίσες με 1;</p></b>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΗ:"
      ],
      "metadata": {
        "id": "A_9Xw95Ufd7k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το α είναι ο ρυθμός εκμάθησης  (μεταξύ 0 και 1), δηλαδή πόσο πρέπει να αλλάξουμε την αρχική τιμή Q(sₜ, aₜ). Εάν α = 0, η τιμή δεν αλλάζει ποτέ, αλλά αν α = 1, η τιμή αλλάζει εξαιρετικά γρήγορα. Για α = 1, η ανταμοιβή και η μέγιστη τιμή στην επόμενη κατάσταση γρήγορα υπερισχύουν της τρέχουσας τιμής. Αυτό όμως δεν είναι καλό, διότι πρέπει να υπάρχει μια ισορροπία μεταξύ  της προηγούμενης και της νέας γνώσης.Συγκεκριμένα, για α=1, η τιμή\n",
        "οδηγεί σε εκμάθηση η οποία δεν κρατάει μνήμη και ως εκ τούτου μπορεί να οδηγήσει σε μια καθαρά στοχαστική συμπεριφορά, άνευ στρατηγικής.\n",
        "\n",
        "Το γ είναι ο παράγοντας έκπτωσης  (μεταξύ 0 και 1), ο οποίος καθορίζει πόσο ενδιαφέρεται ο πράκτορας για τις μελλοντικές ανταμοιβές σε σύγκριση με τις άμεσες. Αν γ = 0, ο πράκτορας εστιάζει μόνο σε άμεσες ανταμοιβές, αλλά αν γ = 1, οποιαδήποτε πιθανή μελλοντική ανταμοιβή έχει την ίδια τιμή με την τρέχουσα.\n",
        "Συγκεκριμένα, για γ=1, δημιουργείται  μια κατάσταση όπου με το πέρασμα των χρονικών βημάτων η αναμενόμενη ανταμοιβή του πράκτορα δε μειώνεται."
      ],
      "metadata": {
        "id": "A74GhR1tfhpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    total_epochs, total_penalties = 0, 0\n",
        "    episodes = 100\n",
        "    rewards = []\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        env.s = 328  # set environment to illustration's state\n",
        "\n",
        "        epochs = 0\n",
        "        penalties, reward = 0, 0\n",
        "\n",
        "        frames = [] # for animation\n",
        "\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            action = env.action_space.sample()\n",
        "            state, reward, done, info = env.step(action)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            if reward == -10:\n",
        "                penalties += 1\n",
        "\n",
        "            # Put each rendered frame into dict for animation\n",
        "            frames.append({\n",
        "                'frame': env.render(mode='ansi'),\n",
        "                'state': state,\n",
        "                'action': action,\n",
        "                'reward': reward\n",
        "                }\n",
        "            )\n",
        "\n",
        "            epochs += 1\n",
        "\n",
        "        total_penalties += penalties\n",
        "        total_epochs += epochs\n",
        "\n",
        "    if episodes==1:\n",
        "        print(\"Timesteps taken: {}\".format(epochs))\n",
        "        print(\"Penalties incurred: {}\".format(penalties))\n",
        "    else:\n",
        "        print(f\"Results after {episodes} episodes:\")\n",
        "        print(f\"Average timesteps per episode: {total_epochs / episodes:.2f}\")\n",
        "        print(f\"Average penalties per episode: {total_penalties / episodes:.2f}\")\n",
        "        rewards = np.asarray(rewards)\n",
        "        print(f\"Average reward per step: {rewards.mean():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2ZpXvY-P7OK",
        "outputId": "f2d429aa-5f4f-4908-e3a8-6a76ae06bb7f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 1.00\n",
            "Average penalties per episode: 0.38\n",
            "Average reward per step: -4.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJk3NTfcXrrA"
      },
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy2Yg8DTXtHW",
        "outputId": "7f360762-95b4-4972-8960-75cc1dce47d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "\n",
        "\"\"\"Training the agent\"\"\"\n",
        "\n",
        "import random\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Hyperparameters\n",
        "alpha = 0.1\n",
        "gamma = 0.6\n",
        "epsilon = 0.1\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample() # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state]) # Exploit learned values\n",
        "\n",
        "        next_state, reward, done,info = env.step(action)\n",
        "\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 13300\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e3f751e54686>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Exploit learned values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/taxi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlastaction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"prob\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action_mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/utils/renderer.py\u001b[0m in \u001b[0;36mrender_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingle_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mrender_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_returns_render\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/taxi.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"single_rgb_array\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render_gui\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/toy_text/taxi.py\u001b[0m in \u001b[0;36m_render_gui\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    374\u001b[0m                     \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"|\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 ):\n\u001b[0;32m--> 376\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian_vert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"|\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian_vert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Υ.Γ. Καταλάθος το έβαλα να ξανατρέξει."
      ],
      "metadata": {
        "id": "-jj47pgrRn51"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxt4fmvGYBOm",
        "outputId": "d345157e-e41e-4dbd-fe18-730824cbd431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "q_table[328]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W9JE9yOYGgP",
        "outputId": "e2d9d068-71b0-44dd-b11b-6eb18db9a5c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
        "\n",
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "for _ in range(episodes):\n",
        "    action = env.action_space.sample()\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        epochs += 1\n",
        "\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")\n",
        "rewards = np.asarray(rewards)\n",
        "print(f\"Average reward per step: {rewards.mean():.2f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 200.0\n",
            "Average penalties per episode: 0.0\n",
            "Average reward per step: -1.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5R7gW1nLj-qE"
      },
      "source": [
        "<b><p>4. Συγκρίνετε τους δύο αλγορίθμους με βάση τις παρακάτω μετρικές</p>\n",
        "<ul>\n",
        "<li>Μέσος αριθμός παραβάσεων ανά επεισόδιο</li>\n",
        "<li>Μέσος αριθμός βημάτων ανά διαδρομή</li>\n",
        "<li>Μέσος αριθμός ανταμοιβών ανά κίνηση</li>\n",
        "</ul>\n",
        "<p>Τις παραπάνω συγκρίσεις να τις κάνετε για 100 επεισόδια.</p>\n",
        "</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΗ:"
      ],
      "metadata": {
        "id": "MY8FkoRdpiFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Βλέπουμε ότι μετά την χρησιμοποιήση του αλγορίθμου Q-learning, ότι όλες οι μετρικές είναι πολύ βελτιωμένες. Το average reward per step από -4.42, που είναι σχεδόν -5, ανεβαίνει στο -1.18. Δηλαδή η μέση ανταμοιβή ανά βήμα από -5 ανεβαίνει στο -1. Παρ΄όλο που δεν φαίνεται καλά στο μάτι -1 ανταμοιβή, δεν μπορούμε να προσπεράσουμε τη σημαντική βελτίωση.\n",
        "\n",
        "Επίσης, τα penalty ανά επεισόδιο πέφτει στο 0, το οποίο είναι πολύ θετικό αφού πλέον δεν γίνονται παραβιάσεις.\n",
        "\n",
        "Ο μέσος αριθμός βημάτων γίνεται 200"
      ],
      "metadata": {
        "id": "uv5bALQ2knG7"
      }
    }
  ]
}