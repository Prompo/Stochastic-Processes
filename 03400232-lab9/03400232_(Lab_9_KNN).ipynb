{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Αλγόριθμος ΚΝΝ\n",
        "\n",
        "Στο παρακάτω παράδειγμα θα εξετάσουμε πως λειτουργεί ο αλγόριθμος Κ-Nearest Neighbors ([KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm?msclkid=0c75f966cf9c11ecbab5cba311a90428)), χρησιμοποιώντας το [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set?msclkid=33057869cf9c11ec8e488a45734cbc68) σε προβλήματα κατηγοριοποίησης και προβλέψεων. Το αρχείο που θα χρησιμοποιήσετε είναι διαθέσιμο [**εδώ**](https://github.com/nkostopoulos/StochasticsLabPublic/blob/master/lab9/KNN/iris.csv).\n",
        "\n",
        "\n",
        "### Ερωτήσεις\n",
        "\n",
        "\n",
        "\n",
        "*   Να περιγράψετε συνοπτικά το τρόπο λειτουργίας του αλγορίθμου ΚΝΝ\n",
        "*   Ποια είναι τα πλεονεκτήματα και τα μειονεκτήματα του συγκεκριμένου αλγορίθμου;\n",
        "*   Ο συγκεκριμένος αλγόριθμος είναι αποδοτικός στην περίπτωση που έχουμε μεγάλο πλήθος χαρακτηριστικών; Τι συμβαίνει στις περιπτώσεις όπου επιλέξουμε μεγάλο ή μικρό k;\n",
        "\n"
      ],
      "metadata": {
        "id": "3Sc2hbFf_5x4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Predictions with k-nearest neighbors on the Iris Flowers Dataset\n",
        "from csv import reader\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "YdDd567UDrAH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a CSV file\n",
        "def load_csv(filename):\n",
        "\tdataset = list()\n",
        "\twith open(filename, 'r') as file:\n",
        "\t\tcsv_reader = reader(file)\n",
        "\t\tfor row in csv_reader:\n",
        "\t\t\tif not row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tdataset.append(row)\n",
        "\treturn dataset\n",
        "\n",
        "# Convert string column to float\n",
        "def str_column_to_float(dataset, column):\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = float(row[column].strip())\n",
        "\n",
        "# Convert string column to integer\n",
        "def str_column_to_int(dataset, column):\n",
        "\tclass_values = [row[column] for row in dataset]\n",
        "\tunique = set(class_values)\n",
        "\tlookup = dict()\n",
        "\tfor i, value in enumerate(unique):\n",
        "\t\tlookup[value] = i\n",
        "\t\tprint('[%s] => %d' % (value, i))\n",
        "\tfor row in dataset:\n",
        "\t\trow[column] = lookup[row[column]]\n",
        "\treturn lookup\n",
        "\n",
        "# Find the min and max values for each column\n",
        "def dataset_minmax(dataset):\n",
        "\tminmax = list()\n",
        "\tfor i in range(len(dataset[0])):\n",
        "\t\tcol_values = [row[i] for row in dataset]\n",
        "\t\tvalue_min = min(col_values)\n",
        "\t\tvalue_max = max(col_values)\n",
        "\t\tminmax.append([value_min, value_max])\n",
        "\treturn minmax\n",
        "\n",
        "# Rescale dataset columns to the range 0-1\n",
        "def normalize_dataset(dataset, minmax):\n",
        "\tfor row in dataset:\n",
        "\t\tfor i in range(len(row)):\n",
        "\t\t\trow[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])"
      ],
      "metadata": {
        "id": "Z0S9ZQ57Dv8-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Αναφέρετε άλλες μεθόδους υπολογισμού της απόστασης\n",
        "\n"
      ],
      "metadata": {
        "id": "2APEXH9DD3S_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΗ:"
      ],
      "metadata": {
        "id": "_Dh2fd4y1muj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Ευκλείδεια απόσταση: η καρτεσιανή απόσταση μεταξύ των δύο σημείων που βρίσκονται στο επίπεδο/υπερεπίπεδο. Η Ευκλείδεια απόσταση μπορεί επίσης να απεικονιστεί ως το μήκος της ευθείας που ενώνει τα δύο σημεία που λαμβάνονται υπόψη. Αυτή η μέτρηση μας βοηθά να υπολογίσουμε την καθαρή μετατόπιση που γίνεται μεταξύ των δύο καταστάσεων ενός αντικειμένου.\n",
        "\n",
        "2.Απόσταση Μανχάταν: η μέτρηση της απόστασης του Μανχάταν χρησιμοποιείται γενικά όταν μας ενδιαφέρει η συνολική απόσταση που διανύει το αντικείμενο αντί για τη μετατόπιση. Αυτή η μέτρηση υπολογίζεται αθροίζοντας την απόλυτη διαφορά μεταξύ των συντεταγμένων των σημείων σε n-διαστάσεις.\n",
        "\n",
        "3.Απόσταση Minkowski: Μπορούμε να πούμε ότι η Ευκλείδεια, όπως και η απόσταση του Μανχάταν, είναι ειδικές περιπτώσεις της απόστασης Minkowski ."
      ],
      "metadata": {
        "id": "GjjqAiuZ1i2O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "R4OjL9p18HL-"
      },
      "outputs": [],
      "source": [
        "# Calculate the Euclidean distance between two vectors\n",
        "def euclidean_distance(row1, row2):\n",
        "\tdistance = 0.0\n",
        "\tfor i in range(len(row1)-1):\n",
        "\t\tdistance += (row1[i] - row2[i])**2\n",
        "\treturn sqrt(distance)\n",
        "\n",
        "# Locate the most similar neighbors\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "\tdistances = list()\n",
        "\tfor train_row in train:\n",
        "\t\tdist = euclidean_distance(test_row, train_row)\n",
        "\t\tdistances.append((train_row, dist))\n",
        "\tdistances.sort(key=lambda tup: tup[1])\n",
        "\tneighbors = list()\n",
        "\tfor i in range(num_neighbors):\n",
        "\t\tneighbors.append(distances[i][0])\n",
        "\treturn neighbors\n",
        "\n",
        "# Make a prediction with neighbors\n",
        "def predict_classification(train, test_row, num_neighbors):\n",
        "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
        "\toutput_values = [row[-1] for row in neighbors]\n",
        "\tprediction = max(set(output_values), key=output_values.count)\n",
        "\treturn prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Δοκιμάστε με τα παρακάτω ζεύγη τιμών και σημειώστε τα αποτελέσματα\n",
        "\n",
        "\n",
        "> *   [4.9, 3.1, 1.5, 0.1]\n",
        "*   [6.9, 3.1, 4.9, 1.5]\n",
        "*   [5.0, 2.0, 3.5, 1.0]\n",
        "*   [5.6, 2.7, 4.2, 1.3]\n",
        "*   [6.3, 3.3, 6.0, 2.5]\n",
        "*   [5.7, 2.9, 4.2, 1.3]\n",
        "*   [5.9, 3.0, 5.1, 1.8]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hIVzwwpSEeE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction with KNN on Iris Dataset\n",
        "filename = 'iris.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# define model parameter\n",
        "num_neighbors = 5\n",
        "# define a new record\n",
        "row = [4.5, 2.3, 1.3, 0.3]\n",
        "# predict the label\n",
        "label = predict_classification(dataset, row, num_neighbors)\n",
        "print('Data=%s, Predicted: %s' % (row, label))"
      ],
      "metadata": {
        "id": "vVci7H7jExef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09480858-03b2-41e8-8dac-bed4d3ca71a0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iris-versicolor] => 0\n",
            "[Iris-virginica] => 1\n",
            "[Iris-setosa] => 2\n",
            "Data=[4.5, 2.3, 1.3, 0.3], Predicted: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_rows = [[4.9, 3.1, 1.5, 0.1],\n",
        "             [6.9, 3.1, 4.9, 1.5],\n",
        "             [5.0, 2.0, 3.5, 1.0],\n",
        "             [5.6, 2.7, 4.2, 1.3],\n",
        "             [6.3, 3.3, 6.0, 2.5],\n",
        "             [5.7, 2.9, 4.2, 1.3],\n",
        "             [5.9, 3.0, 5.1, 1.8]]\n",
        "\n",
        "for row in test_rows:\n",
        "    label = predict_classification(dataset, row, num_neighbors)\n",
        "    print('Data=%s, Predicted: %s' % (row, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wRSgZjn5tJa",
        "outputId": "28e5a5b6-67d9-4905-de9f-3a62e9fc6342"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data=[4.9, 3.1, 1.5, 0.1], Predicted: 2\n",
            "Data=[6.9, 3.1, 4.9, 1.5], Predicted: 0\n",
            "Data=[5.0, 2.0, 3.5, 1.0], Predicted: 0\n",
            "Data=[5.6, 2.7, 4.2, 1.3], Predicted: 0\n",
            "Data=[6.3, 3.3, 6.0, 2.5], Predicted: 1\n",
            "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 0\n",
            "Data=[5.9, 3.0, 5.1, 1.8], Predicted: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Δοκιμάστε να υπολογίσετε την απόσταση με τη μετρική [Manhattan](https://iq.opengenus.org/manhattan-distance/#:~:text=Manhattan%20distance%20is%20a%20distance%20metric%20between%20two,the%20measures%20in%20all%20dimensions%20of%20two%20points.?msclkid=50bbf70ecfa011ec91862d6b9263d761) για τα ζέυγη που σας έχουν δοθεί. Παρατηρείτε κάποια διαφοροποίηση ως προς τα αποτελέσματα;\n",
        "\n"
      ],
      "metadata": {
        "id": "9__nuQqGGyN4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def manhattan_distance(row1, row2):\n",
        "    distance = 0.0\n",
        "    for i in range(len(row1) - 1):  # Υποθέτουμε ότι η τελευταία στήλη είναι η ετικέτα/κατηγορία\n",
        "        distance += abs(row1[i] - row2[i])\n",
        "    return distance\n",
        "\n",
        "# Locate the most similar neighbors\n",
        "def get_neighbors(train, test_row, num_neighbors):\n",
        "\tdistances = list()\n",
        "\tfor train_row in train:\n",
        "\t\tdist = manhattan_distance(test_row, train_row)\n",
        "\t\tdistances.append((train_row, dist))\n",
        "\tdistances.sort(key=lambda tup: tup[1])\n",
        "\tneighbors = list()\n",
        "\tfor i in range(num_neighbors):\n",
        "\t\tneighbors.append(distances[i][0])\n",
        "\treturn neighbors\n",
        "\n",
        "# Make a prediction with neighbors\n",
        "def predict_classification(train, test_row, num_neighbors):\n",
        "\tneighbors = get_neighbors(train, test_row, num_neighbors)\n",
        "\toutput_values = [row[-1] for row in neighbors]\n",
        "\tprediction = max(set(output_values), key=output_values.count)\n",
        "\treturn prediction"
      ],
      "metadata": {
        "id": "NYE7CniJ5NM_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction with KNN on Iris Dataset\n",
        "filename = 'iris.csv'\n",
        "dataset = load_csv(filename)\n",
        "for i in range(len(dataset[0])-1):\n",
        "\tstr_column_to_float(dataset, i)\n",
        "# convert class column to integers\n",
        "str_column_to_int(dataset, len(dataset[0])-1)\n",
        "# define model parameter\n",
        "num_neighbors = 5\n",
        "# define a new record\n",
        "row = [4.5, 2.3, 1.3, 0.3]\n",
        "# predict the label\n",
        "label = predict_classification(dataset, row, num_neighbors)\n",
        "print('Data=%s, Predicted: %s' % (row, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7145nPc55WqB",
        "outputId": "db8fda06-a1b9-41c4-c842-68acd7416948"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Iris-versicolor] => 0\n",
            "[Iris-virginica] => 1\n",
            "[Iris-setosa] => 2\n",
            "Data=[4.5, 2.3, 1.3, 0.3], Predicted: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_rows = [[4.9, 3.1, 1.5, 0.1],\n",
        "             [6.9, 3.1, 4.9, 1.5],\n",
        "             [5.0, 2.0, 3.5, 1.0],\n",
        "             [5.6, 2.7, 4.2, 1.3],\n",
        "             [6.3, 3.3, 6.0, 2.5],\n",
        "             [5.7, 2.9, 4.2, 1.3],\n",
        "             [5.9, 3.0, 5.1, 1.8]]\n",
        "\n",
        "for row in test_rows:\n",
        "    label = predict_classification(dataset, row, num_neighbors)\n",
        "    print('Data=%s, Predicted: %s' % (row, label))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id0alCI58mea",
        "outputId": "73ffdc9f-86e8-4ab2-f758-40dbee2439ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data=[4.9, 3.1, 1.5, 0.1], Predicted: 2\n",
            "Data=[6.9, 3.1, 4.9, 1.5], Predicted: 0\n",
            "Data=[5.0, 2.0, 3.5, 1.0], Predicted: 0\n",
            "Data=[5.6, 2.7, 4.2, 1.3], Predicted: 0\n",
            "Data=[6.3, 3.3, 6.0, 2.5], Predicted: 1\n",
            "Data=[5.7, 2.9, 4.2, 1.3], Predicted: 0\n",
            "Data=[5.9, 3.0, 5.1, 1.8], Predicted: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΣΧΟΛΙΟ:"
      ],
      "metadata": {
        "id": "oQ5DEz-e9G2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Με βάση τα παραπάνω αποτελέσματα, δεν υπάρχει κάποια αλλαγή, δηλαδή οι 2 αλγόριθμοι δώσανε το ίδιο αποτέλεσμα."
      ],
      "metadata": {
        "id": "tXZz58jh84S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΕΙΣ ΘΕΩΡΙΑΣ:"
      ],
      "metadata": {
        "id": "LOdOencGoB37"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i)\n",
        "\n",
        "Ο αλγόριθμος (K-NN) είναι ένας ευέλικτος και ευρέως χρησιμοποιούμενος αλγόριθμος μηχανικής μάθησης που χρησιμοποιείται κυρίως για την απλότητα και την ευκολία εφαρμογής του. Δεν απαιτεί υποθέσεις σχετικά με την υποκείμενη διανομή δεδομένων. Μπορεί επίσης να χειριστεί τόσο αριθμητικά όσο και κατηγορικά δεδομένα, καθιστώντας το μια ευέλικτη επιλογή για διάφορους τύπους συνόλων δεδομένων σε εργασίες ταξινόμησης και παλινδρόμησης. Είναι μια μη παραμετρική μέθοδος που κάνει προβλέψεις με βάση την ομοιότητα των σημείων δεδομένων σε ένα δεδομένο σύνολο δεδομένων. Το K-NN είναι λιγότερο ευαίσθητο σε ακραίες τιμές σε σύγκριση με άλλους αλγόριθμους.\n",
        "\n",
        "Ο αλγόριθμος K-NN λειτουργεί βρίσκοντας τους K πλησιέστερους γείτονες σε ένα δεδομένο σημείο δεδομένων με βάση μια μέτρηση απόστασης, όπως η Ευκλείδεια απόσταση. Η κλάση ή η τιμή του σημείου δεδομένων καθορίζεται στη συνέχεια από την πλειοψηφία ή τον μέσο όρο των γειτόνων Κ. Αυτή η προσέγγιση επιτρέπει στον αλγόριθμο να προσαρμόζεται σε διαφορετικά μοτίβα και να κάνει προβλέψεις με βάση την τοπική δομή των δεδομένων.\n",
        "\n",
        "Για να προσδιορίσουμε τις πλησιέστερες ομάδες ή τα πλησιέστερα σημεία για ένα σημείο στον αλγόριθμο αυτό χρειαζόμαστε κάποια μέτρηση. Για το σκοπό αυτό, χρησιμοποιούμε τις παρακάτω μετρήσεις απόστασης:\n",
        "\n",
        "\n",
        "1. Ευκλείδεια απόσταση: η καρτεσιανή απόσταση μεταξύ των δύο σημείων που βρίσκονται στο επίπεδο/υπερεπίπεδο. Η Ευκλείδεια απόσταση μπορεί επίσης να απεικονιστεί ως το μήκος της ευθείας που ενώνει τα δύο σημεία που λαμβάνονται υπόψη. Αυτή η μέτρηση μας βοηθά να υπολογίσουμε την καθαρή μετατόπιση που γίνεται μεταξύ των δύο καταστάσεων ενός αντικειμένου.\n",
        "\n",
        "2. Απόσταση Μανχάταν: η μέτρηση της απόστασης του Μανχάταν χρησιμοποιείται γενικά όταν μας ενδιαφέρει η συνολική απόσταση που διανύει το αντικείμενο αντί για τη μετατόπιση. Αυτή η μέτρηση υπολογίζεται αθροίζοντας την απόλυτη διαφορά μεταξύ των συντεταγμένων των σημείων σε n-διαστάσεις.\n",
        "\n",
        "3. Απόσταση Minkowski: Μπορούμε να πούμε ότι η Ευκλείδεια, όπως και η απόσταση του Μανχάταν, είναι ειδικές περιπτώσεις της απόστασης Minkowski .\n",
        "\n",
        "Συνοπτικά: για να εφαρμόσουμε τον αλγόριθμο K-NN πρέπει να εφαρμόσουμε τα παρακάτω βήματα:\n",
        "\n",
        "1.  Επιλογή της βέλτιστης τιμής του K: Το K αντιπροσωπεύει τον αριθμό των πλησιέστερων γειτόνων που πρέπει να ληφθούν υπόψη κατά την πρόβλεψη.\n",
        "2. Υπολογισμός απόστασης: Για τη μέτρηση της ομοιότητας μεταξύ σημείων δεδομένων στόχου και εκπαίδευσης, χρησιμοποιείται η Ευκλείδεια απόσταση. Η απόσταση υπολογίζεται μεταξύ καθενός από τα σημεία δεδομένων στο σύνολο δεδομένων και το σημείο στόχου.\n",
        "3.Εύρεση πλησιέστερων γειτόνων: Τα k σημεία δεδομένων με τις μικρότερες αποστάσεις από το σημείο στόχο είναι οι πλησιέστεροι γείτονες\n",
        "4. Απόφαση για ταξινόμηση ή λήψη μέσου όρου για παλινδρόμηση:\n",
        "Στο πρόβλημα ταξινόμησης, οι ετικέτες κλάσης του καθορίζονται με πλειοψηφία. Η κλάση με τις περισσότερες εμφανίσεις μεταξύ των γειτόνων γίνεται η προβλεπόμενη κλάση για το σημείο δεδομένων στόχου.\n",
        "Στο πρόβλημα παλινδρόμησης, η ετικέτα κλάσης υπολογίζεται λαμβάνοντας τον μέσο όρο των τιμών στόχου των K πλησιέστερων γειτόνων. Η υπολογισμένη μέση τιμή γίνεται η προβλεπόμενη έξοδος για το σημείο δεδομένων στόχου."
      ],
      "metadata": {
        "id": "9GTsimw1odRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii)\n",
        "\n",
        "Πλεονεκτήματα του αλγορίθμου KNN\n",
        "\n",
        "1. Εύκολο στην εφαρμογή καθώς η πολυπλοκότητα του αλγορίθμου δεν είναι τόσο υψηλή.\n",
        "2. Προσαρμόζεται εύκολα --> Σύμφωνα με τη λειτουργία του αλγόριθμου KNN, αποθηκεύει όλα τα δεδομένα στην αποθήκευση μνήμης και επομένως κάθε φορά που προστίθεται ένα νέο παράδειγμα ή σημείο δεδομένων, ο αλγόριθμος προσαρμόζεται σύμφωνα με αυτό το νέο παράδειγμα και έχει τη συμβολή του και στις μελλοντικές προβλέψεις .\n",
        "3. Λίγες υπερπαράμετροι – Οι μόνες παράμετροι που απαιτούνται στην εκπαίδευση ενός αλγόριθμου KNN είναι η τιμή του k και η επιλογή της μέτρησης απόστασης που θα θέλαμε να επιλέξουμε από τη μέτρηση αξιολόγησής μας.\n",
        "\n",
        "Μειονεκτήματα του αλγορίθμου KNN\n",
        "\n",
        "1. Δεν κλιμακώνεται --> Όπως έχουμε ακούσει για αυτό ότι ο αλγόριθμος KNN θεωρείται επίσης ένας αλγόριθμος Lazy. Η κύρια σημασία αυτού του όρου είναι ότι απαιτεί μεγάλη υπολογιστική ισχύ καθώς και αποθήκευση δεδομένων. Αυτό καθιστά αυτόν τον αλγόριθμο τόσο χρονοβόρο όσο και εξαντλητικό των πόρων.\n",
        "2. Curse of Dimensionality – Υπάρχει ένας όρος γνωστός ως φαινόμενο κορύφωσης σύμφωνα με αυτό, ο αλγόριθμος KNN επηρεάζεται από την κατάρα της διάστασης που υποδηλώνει ότι ο αλγόριθμος αντιμετωπίζει έναν δύσκολο χρόνο να ταξινομήσει σωστά τα σημεία δεδομένων όταν η διάσταση είναι πολύ υψηλή.\n",
        "3. Επιρρεπής σε υπερπροσαρμογή – Καθώς ο αλγόριθμος επηρεάζεται λόγω της κατάρας της διαστασιοποίησης, είναι επίσης επιρρεπής στο πρόβλημα της υπερπροσαρμογής. Ως εκ τούτου, γενικά η επιλογή χαρακτηριστικών καθώς και οι τεχνικές μείωσης διαστάσεων εφαρμόζονται για την αντιμετώπιση αυτού του προβλήματος."
      ],
      "metadata": {
        "id": "DlsuD98hoFpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii)\n",
        "\n",
        "Ο αλγόριθμος k-Nearest Neighbors (KNN) μπορεί να παρουσιάσει σημαντικά ζητήματα απόδοσης όταν εφαρμόζεται σε σύνολα δεδομένων με μεγάλο πλήθος χαρακτηριστικών, φαινόμενο γνωστό ως «curse of dimensionality» (κατάρα της διαστατικότητας).\n",
        "\n",
        "Curse of Dimensionality: Όταν ο αριθμός των χαρακτηριστικών (διαστάσεων) είναι πολύ μεγάλος, η απόσταση μεταξύ των σημείων δεδομένων μπορεί να γίνεται λιγότερο διακριτή. Οι περισσότερες μετρικές απόστασης, όπως η Ευκλείδια απόσταση, γίνονται λιγότερο αποτελεσματικές, καθώς οι αποστάσεις τείνουν να συγκλίνουν σε παρόμοιες τιμές. Αυτό καθιστά δύσκολη την αναγνώριση των πλησιέστερων γειτόνων.\n",
        "\n",
        "Ο KNN απαιτεί τον υπολογισμό της απόστασης μεταξύ του σημείου προς ταξινόμηση και κάθε σημείου στο σύνολο εκπαίδευσης. Όταν τα χαρακτηριστικά είναι πολλά, αυτός ο υπολογισμός γίνεται πιο απαιτητικός σε πόρους και χρόνο.\n",
        "\n",
        "Το πρόβλημα αυτό μπορεί να αντιμετωπιστεί με την εφαρμογή τεχνικών μείωσης των διαστάσεων, όπως η Ανάλυση Κύριων Συνιστωσών (PCA) ή η επιλογή χαρακτηριστικών, για να βελτιωθεί η απόδοση του KNN σε σύνολα δεδομένων με μεγάλο πλήθος χαρακτηριστικών.\n",
        "\n",
        "\n",
        "Μικρή τιμή του k(k=1 ή 2):\n",
        "\n",
        "Πλεονεκτήματα: Ο αλγόριθμος μπορεί να είναι πιο ευαίσθητος και να ανιχνεύει καλύτερα τις τοπικές δομές στα δεδομένα. Μπορεί να επιτύχει υψηλή ακρίβεια σε καλά διαχωρισμένα σύνολα δεδομένων.\n",
        "\n",
        "Μειονεκτήματα: Αυξάνεται η ευαισθησία στον θόρυβο και τις ανωμαλίες(δλαδή πληροφορίες ή σημεία δεδομένων που είναι άσχετα ή τυχαία και δεν ακολουθούν τα βασικά πρότυπα ή τάσεις που παρουσιάζει το υπόλοιπο σύνολο δεδομένων). Τα αποτελέσματα μπορεί να είναι ασταθή, καθώς ένα μεμονωμένο σημείο θορύβου μπορεί να επηρεάσει σημαντικά την απόφαση ταξινόμησης.\n",
        "\n",
        "\n",
        "Μεγάλη τιμή του k (k = 15 ή 20):\n",
        "\n",
        "Πλεονεκτήματα: Ο αλγόριθμος γίνεται πιο σταθερός και λιγότερο επηρεασμένος από τον θόρυβο. Οι αποφάσεις ταξινόμησης βασίζονται σε μεγαλύτερο αριθμό γειτόνων, γεγονός που μπορεί να βελτιώσει τη γενίκευση.\n",
        "\n",
        "Μειονεκτήματα: Η αυξημένη τιμή του k μπορεί να προκαλέσει υπερβολική εξομάλυνση των αποφάσεων ταξινόμησης, μειώνοντας την ικανότητα του αλγορίθμου να αναγνωρίζει λεπτές τοπικές δομές και διαφοροποιήσεις στα δεδομένα. Αυτό μπορεί να οδηγήσει σε χαμηλότερη ακρίβεια σε σύνολα δεδομένων με πολύπλοκα πρότυπα."
      ],
      "metadata": {
        "id": "JZDVcQGsyRM8"
      }
    }
  ]
}