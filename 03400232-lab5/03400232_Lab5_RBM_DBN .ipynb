{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1><b>Restricted Boltzmann Machine και Deep Belief Network</b></h1>\n",
        "<p align=\"justify\">Στην συγκεκριμένη άσκηση θα μελετήσετε τον τρόπο λειτουργίας μιας <i>RBM (<a href=\"https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine\">Restricted Boltzmann Machine</a>)</i> καθώς και των <i>DBN (<a href=\"https://en.wikipedia.org/wiki/Deep_belief_network\">Deep Belief Network</a>)</i>, χρησιμοποιώντας το έτοιμο πρόγραμμα που σας δίνεται.Το συγκεκριμένο πρόγραμμα αξιοποιεί το <a href=\"https://en.wikipedia.org/wiki/MNIST_database\">dataset του <i>MNIST</i></a>, όπου είναι μια μεγάλη βάση δεδομένων με χειρόγραφα ψηφία που χρησιμοποιείται συνήθως για την εκπαίδευση διαφόρων συστημάτων επεξεργασίας εικόνας. Για την άσκηση, θα πρέπει να χρησιμοποιήσετε το αρχείο <i>mnist_original.mat</i>, το οποίο είναι διαθέσιμο από <a href=\"https://www.kaggle.com/datasets/avnishnish/mnist-original?resource=download\">εδώ</a>.</p>\n",
        "<p align=\"justify\">Μία αρκετά σημαντική εφαρμογή της <i>RBM</i> είναι η εξαγωγή χαρακτηριστικών (feature representation) από ένα dataset με σκοπό την αναπαράσταση της εισόδου (ορατοί νευρώνες) με ένα διάνυσμα μικρότερης διάστασης (κρυφοί νευρώνες). Στη συγκεκριμένη άσκηση θα συγκρίνετε την ακρίβεια ενός ταξινομητή ψηφίων με τη χρήση του αλγορίθμου <i>Logistic Regression</i>, όταν εκείνος δέχεται ως είσοδο το dataset (i) χωρίς να έχει υποστεί επεξεργασία από το <i>RBM</i>, (ii) αφου υποστεί επεξεργασία από το <i>RBM</i>, (iii) με τη χρήση <i>DBN</i>, δηλαδή δύο stacked <i>RBM</i>.</p>\n",
        "<p align=\"justify\"> Με βάση τον κώδικα που σας έχει δοθεί, καλείστε να απαντήσετε στα παρακάτω ερωτήματα:</p>\n",
        "<ul>\n",
        "<li>Να περιγράψετε σύντομα τον τρόπο λειτουργίας μιας <i>RBM</i>. Τι διαφορές έχει σε σχέση με μία <i> Μηχανή Boltzmann</i>;</li>\n",
        "<li>Ποια είναι η λογική των <i>DBN</i> και σε τι προβλήματα τα αξιοποιούμε;</li>\n",
        "<li>Να αναφέρετε τις βασικότερες εφαρμογές των <i>RBM</i> και <i>DBN</i>.</li>\n",
        "<li>Εκτός από <i>RBM</i>, τι άλλα μοντέλα μπορούν να χρησιμοποιηθούν για να δημιουργήσουν <i>DBN</i>.</li>\n",
        "<li>Συγκρίνετε τα αποτελέσματα της ταξινόμησης με τον αλγόριθμo <i>Logistic Regression</i> χωρίς τη χρήση <i>RBM</i> σε σχέση με τα αποτελέσματα της ταξινόμησης που έχει χρησιμοποιηθεί η <i>RBM</i> καθώς και με αυτή όπου χρησιμοποιούνται <i>RBM</i> και <i>DBN</i> για την εξαγωγή των χαρακτηριστικών. Τι παρατηρείτε ως προς την ακρίβεια των αποτελεσμάτων;</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "B5q7C447X8Up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ΑΠΑΝΤΗΣΕΙΣ:\n"
      ],
      "metadata": {
        "id": "KACMMXGCHZl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) Η Restricted Boltzman Machine είναι ένα μοντέλο μη κατευθυνόμενου γράφου που υπάγεται στους αλγορίθμους βαθιάς μάθησης. Παίζει ένα σημαντικό ρόλο στη μείωση διαστάσεων , ταξινόμηση και παλλινδρόμηση. Η RBM είναι ένα βασικό κομμάτι του Deep-Belief network. Έχει 2 στρώματα νευρωνικών δικτύων. Το 1ο στρώμα της RBM καλείται visiblle layer (ορατό στρώμα) ή input layer (στρώμα εισόδου), ενώ το 2ο στρώμα είναι hidden layer (κρυμμένο στρώμα).\n",
        "Στην RBM, οι διασυνδέσεις μεταξύ ορατών κόμβων και κρυμμένων κόμβων δημιουργούνται με τη χρήση συμμετρικών βαρών. Συγκεκριμένα, όλα οι hidden nodes συνδέονται με  όλους τους visible nodes, αλλά δεν υπάρχει σύνδεση μεταξύ κόμβων ίδιου στρώματος. Δηλαδή, δεν μπορούν να συνδεθούν μεταξύ τους 2 κόμβοι από το hidden layer, όπως επίσης δεν μπορούν να συνδεθούν μεταξύ τους κόμβοι από το visible layer.\n",
        "Κάθε κόμβος δέχεται και επεξεργαζεται την είσοδο και παίρνει στοχαστικές αποφάσεις σχετικά με το αν θα μεταδώσει το μηνύμα ή όχι.\n",
        "Ο αλγόριθμος αυτός ανήκει στη κατηγορία unsupervised learning, καθώς μέσα από τη διαδικασία να πηγαίνει μπρος και πίσω μεταξύ του ορατού (visible) και του κρυμμένου (hidden) στρώματος, η RBM ανακατασκευάζει τα δεδομένα.\n",
        "Ο αλγόριθμος Restricted Boltzman Machine λέγεται Restricted εξαιτίας του περιορισμού σύνδεσης μεταξύ νευρώνων ίδιου στρώματος. Ο αλγόριθμος RBM βασίζεται σε έναν άλλον αλγόριθμο βαθιάς μάθησης που λέγεται Boltzman Machine.\n",
        "Η κύρια διαφορά μεταξύ Restricted Boltzman Machine και Boltzman Machine είναι ότι στη Boltzman Machine, οι νευρώνες(nodes) δεν συνδέονται μόνο με τους νευρώνες(nodes) των άλλων στρωμάτων(layers), αλλά συνδέονται και μεταξύ τους. Δηλαδή, συνδέονται νευρώνες που βρίσκονται στο ίδιο layer. Είναι απαραίτητο κάθε νευρώνας του συστήματος να συνδέεται με όλους τους νευρώνες του συστήματος.Άρα, σε αντίθεση με τη Restricted Boltzman Machine, η Boltzman Machine έχει ασυμμετρία στα βάρη.\n",
        "Επίσης, όταν η είσοδος αυξάνεται επ΄αόριστον, το feature δεν έχει αποδοτική επιρροή στη δομή. Για να αντιμετωπίσουμε αυτή τη κατάσταση δημιουργήθηκε ο αλγόριθμος Restricted Boltzman Machine. Η RBM προσπαθεί να εκπαιδεύσει το δίκτυο να αντιγράφει την είσοδο πιθανολογικά και να μαθαίνει τη πιθανότητα κατανομής της εισόδου.\n",
        "\n"
      ],
      "metadata": {
        "id": "ztelRMKTH2Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Τα Deep Belief Networks (DBNs) είναι νευρωνικά δίκτυα με πολλαπλά Restricted Boltzman Machine layers. Δηλαδή. με πολλά στρώματα RBM.\n",
        "Επίσης, στα DBNs , η είσοδος του επόμενου στρώματος προέρχεται από την έξοδο του προηγούμενου στρώματος. Τα DBNs υιοθετούν μια ιεραρχικά μη επιβλεπόμενη άπληστη μέθοδο προεκπαίδευσης για να προεκπαιδεύσει ιεραρχικά κάθε Restricted Boltzman Machine.\n",
        "Τα DBNs αναπαριστούν πιο περίπλοκα και αφηρημένα δεδομένα. Συγκεκριμένα, επειδή τα DBNs είναι επί των πλήστων παραγωγικά μοντέλα, αυτό που κάνουν είναι ότι βελτιώνουν την αναπαράσταση του feature με σκοπό να προσεγγίσουμε τη target function.\n",
        "Με τη χρήση των DBNs, απλά features που παρέχονται στο 1ο στρώμα τοποθετούνται σε πιο περίπλοκες και αφηρημένες αναπαραστάσεις των δεδομένων. Χρησιμοποιούμε DBNs για να αναλύσουμε και να ταυτοποιήσουμε αντικείμενα στον πραγματικό κόσμο.\n",
        "Τα RBM μπορούν να χρησιμοποιηθούν για την κατασκευή και την εκαίδευση των DBN που έχουν μεγαλύτερη αναπαραστατική ισχύ από ένα μόνο RBM. Για να εκπαιδεύσουμε ένα DBN, το εκπαιδεύουμε στρώμα στρώμα , ξεκινώντας από το πρώτο κρυφό στρώμα. Μόλις εκπαιδευτεί ένα συγκεκριμένο στρώμα, τα βάρη και τα biases(μεροληψίες) παγώνουν  για αυτό το στρώμα  και για το επόμενο κρυφό στρώμα πάνω από αυτό που εκπαιδεύονται. Οι τιμές που προκύπτουν από το προηγούμενο κρυφό στρώμα χρησιμοποιούνται ως δεδομένα για να εκπαιδεύσουν το επόμενο κρυφό στρώμα. Οι τιμές αυτές προκύπτουν από την είσοδο της εικόνας σε ένα visible layer. Η διαδικασία επαναλαμβάνεται μέχρι να εκπαιδευτεί και το τελευταίο στρώμα."
      ],
      "metadata": {
        "id": "nvpjOf8VTTRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iii) Εφαρμογές RBM και DBN:\n",
        "Αναγνώριση προτύπων σε εικόνες, ήχο , κείμενο και άλλα είδη δεδομένων.\n",
        "Ταξινόμηση δεδομένων.\n",
        "Συσταδοποίηση: Ομαδοποίηση δεδομένων σε συστάδες βάσει των ομοιοτήτων τους.\n",
        "Μείωση διαστάσεων, επιλέγοντας εξόδους μικρότερου μεγέθους σε σχέση με την είσοδο με την είσοδο, δηλαδή το σύνολο των δειγματικών σημείων.\n",
        "\n",
        "Εφαρμογή RBM - Αναπαραγωγή Χειρόγραφων Αριθμών :\n",
        "Προβλήματα γενίκευσης από υπεραπλούστευση διαδικασίας μάθησης:\n",
        "Λανθασμένη αναπαραγωγή χειρόγραφου αριθμού 3 από RBM με δείγμα\n",
        "μάθησης αποκλειστικά με χειρόγραφα στοιχεία αριθμού 2\n",
        "\n",
        "Δείγμα Μάθησης RBM με Ενσωμάτωση Label σε Εικόνες\n",
        "Δείγμα Μάθησης Εικόνων με προσθήκη metadata: Κωδικοποίηση κλάσης σαν label\n",
        "και ενσωμάτωση σε δειγματικά στοιχεία μάθησης που κλειδώνονται στην αρχική\n",
        "κατάσταση των ορατών νευρώνων της RBM\n",
        "Ταξινόμηση Δείγματος Test\n",
        "Είσοδος εικόνας test χωρίς label και αναπαραγωγή της στην τελική κατάσταση των\n",
        "ορατών νευρώνων της RBM με συμπλήρωση πληροφορίας κλάσης σύμφωνα με τις\n",
        "στατιστικές εκτιμήσεις που προέκυψαν από τη διαδικασία μάθησης"
      ],
      "metadata": {
        "id": "dw5eV3D_rWvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "iv) Άλλα μοντέλα που μπορούν να χρησιμοποιηθούν για τη δημιουργία DBN είναι:\n",
        "Autoencoders: Οι Autoencoders είναι νευρωνικά δίκτυα που χρησιμοποιούνται για αυτόματη εκμάθηση χαρακτηριστικών. Αποτελούνται από έναν κωδικοποιητή (encoder) που μετατρέπει την είσοδο σε μια πιο συμπαγή αναπαράσταση (συνήθως μικρότερη διάσταση) και έναν αποκωδικοποιητή (decoder) που αποκωδικοποιεί την αναπαράσταση πίσω στην αρχική της μορφή. Μπορούν να χρησιμοποιηθούν ως βάση για τη δημιουργία DBN.\n",
        "\n",
        "Convolutional Neural Networks (CNNs): Οι Convolutional Neural Networks είναι κατάλληλες για την επεξεργασία εικόνων και χαρακτηρίζονται από τη χρήση φίλτρων συνέλιξης που μπορούν να εντοπίσουν χαρακτηριστικά σε διαφορετικά επίπεδα πυκνότητας. Μπορούν επίσης να χρησιμοποιηθούν ως προεπεξεργαστές για την εξαγωγή χαρακτηριστικών πριν από τη χρήση ενός DBN.\n",
        "\n",
        "Stacked Denoising Autoencoders (SDAEs): Οι Stacked Denoising Autoencoders είναι μια εξέλιξη των απλών Autoencoders, στην οποία κάθε στρώμα του δικτύου εκπαιδεύεται να απομακρύνει θόρυβο από την είσοδο. Η διαδικασία αυτή μπορεί να βοηθήσει στην εκπαίδευση πιο σταθερών και γενικευμένων χαρακτηριστικών, κάτι που είναι σημαντικό για τη δημιουργία DBN.\n",
        "\n",
        "Generative Adversarial Networks (GANs): Οι Generative Adversarial Networks είναι μοντέλα που αποτελούνται από δύο ανταγωνιστικά δίκτυα, έναν γεννήτορα και έναν διακριτή, τα οποία εκπαιδεύονται αμοιβαία. Οι GANs μπορούν να χρησιμοποιηθούν για τη δημιουργία νέων δεδομένων με βάση μια δεδομένη κατανομή πιθανοτήτων και μπορεί να χρησιμοποιηθούν επίσης ως στρώματα προεπεξεργασίας για DBN."
      ],
      "metadata": {
        "id": "b4SR_Zw4szsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "# source: https://devdreamz.com/question/905929-stacking-rbms-to-create-deep-belief-network-in-sklearn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.io import loadmat\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import BernoulliRBM\n",
        "from sklearn.base import clone\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def norm(arr):\n",
        "    arr = arr.astype(float)\n",
        "    arr -= arr.min()\n",
        "    arr /= arr.max()\n",
        "    return arr\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # load MNIST data set\n",
        "    mnist = loadmat(\"mnist-original.mat\")\n",
        "    X, Y = mnist[\"data\"].T, mnist[\"label\"][0]\n",
        "\n",
        "    # normalize inputs to 0-1 range\n",
        "    X = norm(X)\n",
        "\n",
        "    # split into train, validation, and test data sets\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,       Y,       test_size=10000, random_state=0)\n",
        "    X_train, X_val,  Y_train, Y_val  = train_test_split(X_train, Y_train, test_size=10000, random_state=0)\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # set hyperparameters\n",
        "\n",
        "    learning_rate = 0.02\n",
        "    total_units   =  800\n",
        "    total_epochs  =   50\n",
        "    batch_size    =  128\n",
        "\n",
        "    C = 100. # optimum for benchmark model according to sklearn docs: https://scikit-learn.org/stable/auto_examples/neural_networks/plot_rbm_logistic_classification.html#sphx-glr-auto-examples-neural-networks-plot-rbm-logistic-classification-py)\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # construct models\n",
        "\n",
        "    # RBM\n",
        "    rbm = BernoulliRBM(n_components=total_units, learning_rate=learning_rate, batch_size=batch_size, n_iter=total_epochs, verbose=1)\n",
        "\n",
        "    # \"output layer\"\n",
        "    logistic = LogisticRegression(C=C, solver='lbfgs', multi_class='multinomial', max_iter=200, verbose=1)\n",
        "\n",
        "    models = []\n",
        "    models.append(Pipeline(steps=[('logistic', clone(logistic))]))                                              # base model / benchmark\n",
        "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('logistic', clone(logistic))]))                        # single RBM\n",
        "    models.append(Pipeline(steps=[('rbm1', clone(rbm)), ('rbm2', clone(rbm)), ('logistic', clone(logistic))]))  # RBM stack / DBN\n",
        "\n",
        "    # --------------------------------------------------------------------------------\n",
        "    # train and evaluate models\n",
        "\n",
        "    for model in models:\n",
        "        # train\n",
        "        model.fit(X_train, Y_train)\n",
        "\n",
        "        # evaluate using validation set\n",
        "        print(\"Model performance:\\n%s\\n\" % (\n",
        "            classification_report(Y_val, model.predict(X_val))))"
      ],
      "metadata": {
        "id": "Z6OQ6-N8ajZJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d01632a-9ee8-41c3-b263-28986b433a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.96      0.96       995\n",
            "         1.0       0.96      0.98      0.97      1121\n",
            "         2.0       0.90      0.90      0.90      1015\n",
            "         3.0       0.90      0.88      0.89      1033\n",
            "         4.0       0.93      0.92      0.92       976\n",
            "         5.0       0.90      0.88      0.89       884\n",
            "         6.0       0.94      0.94      0.94       999\n",
            "         7.0       0.92      0.93      0.92      1034\n",
            "         8.0       0.88      0.86      0.87       923\n",
            "         9.0       0.89      0.90      0.89      1020\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n",
            "\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -139.68, time = 14.82s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -119.41, time = 20.84s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -105.27, time = 20.60s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -99.22, time = 19.90s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -92.49, time = 20.50s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -87.88, time = 22.46s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -84.93, time = 29.52s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -80.96, time = 23.95s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -80.49, time = 23.24s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -80.07, time = 19.64s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -77.54, time = 20.40s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -76.04, time = 19.82s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -76.01, time = 25.10s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -73.83, time = 25.81s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -74.04, time = 24.12s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -72.10, time = 26.82s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -71.80, time = 22.78s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -71.43, time = 22.45s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -69.94, time = 30.37s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -68.41, time = 25.40s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -69.82, time = 21.53s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -68.28, time = 22.86s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -67.88, time = 29.67s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -67.72, time = 27.50s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -67.68, time = 25.69s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -65.36, time = 22.32s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -66.12, time = 25.39s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -67.05, time = 29.43s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -64.22, time = 30.78s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -63.46, time = 24.94s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -64.87, time = 29.76s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -64.58, time = 31.32s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -65.10, time = 22.39s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -64.23, time = 20.78s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -64.05, time = 21.06s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -64.29, time = 21.05s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -63.56, time = 21.01s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -64.16, time = 27.37s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -65.61, time = 31.76s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -62.64, time = 25.77s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -62.12, time = 20.53s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -62.89, time = 21.04s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -61.69, time = 20.77s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -62.73, time = 20.14s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -62.91, time = 20.54s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -61.87, time = 20.22s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -62.12, time = 19.81s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -62.17, time = 20.41s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -60.92, time = 22.11s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -61.76, time = 27.66s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.98      0.98       995\n",
            "         1.0       0.99      0.99      0.99      1121\n",
            "         2.0       0.96      0.97      0.97      1015\n",
            "         3.0       0.97      0.97      0.97      1033\n",
            "         4.0       0.98      0.97      0.97       976\n",
            "         5.0       0.98      0.96      0.97       884\n",
            "         6.0       0.99      0.99      0.99       999\n",
            "         7.0       0.97      0.98      0.97      1034\n",
            "         8.0       0.96      0.96      0.96       923\n",
            "         9.0       0.96      0.96      0.96      1020\n",
            "\n",
            "    accuracy                           0.97     10000\n",
            "   macro avg       0.97      0.97      0.97     10000\n",
            "weighted avg       0.97      0.97      0.97     10000\n",
            "\n",
            "\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -139.47, time = 15.44s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -119.80, time = 21.87s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -105.25, time = 25.74s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -98.30, time = 30.67s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -91.45, time = 29.66s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -87.90, time = 29.70s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -85.23, time = 29.14s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -83.86, time = 31.36s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -80.15, time = 23.58s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -78.44, time = 23.18s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -76.62, time = 20.71s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -76.15, time = 22.98s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -73.65, time = 23.18s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -73.24, time = 26.08s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -72.36, time = 20.07s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -72.65, time = 23.61s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -71.05, time = 21.04s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -71.88, time = 19.90s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -68.54, time = 20.86s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -69.74, time = 25.23s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -68.39, time = 20.86s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -69.40, time = 20.03s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -67.83, time = 24.41s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -67.50, time = 28.68s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -65.83, time = 29.61s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -66.78, time = 30.34s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -66.74, time = 30.35s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -67.81, time = 25.99s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -65.35, time = 19.91s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -65.39, time = 20.54s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -65.05, time = 20.89s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -64.86, time = 20.20s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -64.69, time = 20.69s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -63.82, time = 19.75s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -63.27, time = 20.83s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -63.59, time = 22.52s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -63.76, time = 28.61s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -63.94, time = 27.54s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -62.87, time = 30.33s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -62.67, time = 27.94s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -63.41, time = 30.53s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -62.66, time = 20.81s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -62.80, time = 20.76s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -61.54, time = 30.31s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -62.46, time = 26.58s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -63.12, time = 28.19s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -62.29, time = 29.97s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -61.88, time = 20.81s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -61.62, time = 23.98s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -61.67, time = 30.96s\n",
            "[BernoulliRBM] Iteration 1, pseudo-likelihood = -283.98, time = 17.10s\n",
            "[BernoulliRBM] Iteration 2, pseudo-likelihood = -257.09, time = 30.58s\n",
            "[BernoulliRBM] Iteration 3, pseudo-likelihood = -238.37, time = 24.52s\n",
            "[BernoulliRBM] Iteration 4, pseudo-likelihood = -228.31, time = 21.71s\n",
            "[BernoulliRBM] Iteration 5, pseudo-likelihood = -225.42, time = 20.62s\n",
            "[BernoulliRBM] Iteration 6, pseudo-likelihood = -220.44, time = 21.44s\n",
            "[BernoulliRBM] Iteration 7, pseudo-likelihood = -218.01, time = 21.67s\n",
            "[BernoulliRBM] Iteration 8, pseudo-likelihood = -214.39, time = 20.79s\n",
            "[BernoulliRBM] Iteration 9, pseudo-likelihood = -213.95, time = 23.32s\n",
            "[BernoulliRBM] Iteration 10, pseudo-likelihood = -213.12, time = 28.84s\n",
            "[BernoulliRBM] Iteration 11, pseudo-likelihood = -209.96, time = 30.00s\n",
            "[BernoulliRBM] Iteration 12, pseudo-likelihood = -208.79, time = 26.39s\n",
            "[BernoulliRBM] Iteration 13, pseudo-likelihood = -208.13, time = 32.40s\n",
            "[BernoulliRBM] Iteration 14, pseudo-likelihood = -208.45, time = 20.91s\n",
            "[BernoulliRBM] Iteration 15, pseudo-likelihood = -207.85, time = 21.62s\n",
            "[BernoulliRBM] Iteration 16, pseudo-likelihood = -203.08, time = 30.31s\n",
            "[BernoulliRBM] Iteration 17, pseudo-likelihood = -200.89, time = 26.56s\n",
            "[BernoulliRBM] Iteration 18, pseudo-likelihood = -202.51, time = 22.87s\n",
            "[BernoulliRBM] Iteration 19, pseudo-likelihood = -199.13, time = 23.50s\n",
            "[BernoulliRBM] Iteration 20, pseudo-likelihood = -202.92, time = 30.40s\n",
            "[BernoulliRBM] Iteration 21, pseudo-likelihood = -198.54, time = 21.11s\n",
            "[BernoulliRBM] Iteration 22, pseudo-likelihood = -201.94, time = 21.05s\n",
            "[BernoulliRBM] Iteration 23, pseudo-likelihood = -199.19, time = 20.29s\n",
            "[BernoulliRBM] Iteration 24, pseudo-likelihood = -201.78, time = 28.11s\n",
            "[BernoulliRBM] Iteration 25, pseudo-likelihood = -198.54, time = 30.13s\n",
            "[BernoulliRBM] Iteration 26, pseudo-likelihood = -198.05, time = 30.05s\n",
            "[BernoulliRBM] Iteration 27, pseudo-likelihood = -195.85, time = 30.02s\n",
            "[BernoulliRBM] Iteration 28, pseudo-likelihood = -196.98, time = 28.44s\n",
            "[BernoulliRBM] Iteration 29, pseudo-likelihood = -196.88, time = 23.76s\n",
            "[BernoulliRBM] Iteration 30, pseudo-likelihood = -199.42, time = 20.29s\n",
            "[BernoulliRBM] Iteration 31, pseudo-likelihood = -195.59, time = 21.32s\n",
            "[BernoulliRBM] Iteration 32, pseudo-likelihood = -196.08, time = 21.08s\n",
            "[BernoulliRBM] Iteration 33, pseudo-likelihood = -196.27, time = 20.22s\n",
            "[BernoulliRBM] Iteration 34, pseudo-likelihood = -194.08, time = 21.17s\n",
            "[BernoulliRBM] Iteration 35, pseudo-likelihood = -193.34, time = 21.36s\n",
            "[BernoulliRBM] Iteration 36, pseudo-likelihood = -197.11, time = 20.21s\n",
            "[BernoulliRBM] Iteration 37, pseudo-likelihood = -196.34, time = 20.87s\n",
            "[BernoulliRBM] Iteration 38, pseudo-likelihood = -195.33, time = 23.16s\n",
            "[BernoulliRBM] Iteration 39, pseudo-likelihood = -194.37, time = 23.68s\n",
            "[BernoulliRBM] Iteration 40, pseudo-likelihood = -194.85, time = 22.06s\n",
            "[BernoulliRBM] Iteration 41, pseudo-likelihood = -192.63, time = 25.49s\n",
            "[BernoulliRBM] Iteration 42, pseudo-likelihood = -194.16, time = 24.95s\n",
            "[BernoulliRBM] Iteration 43, pseudo-likelihood = -194.12, time = 22.06s\n",
            "[BernoulliRBM] Iteration 44, pseudo-likelihood = -195.24, time = 21.10s\n",
            "[BernoulliRBM] Iteration 45, pseudo-likelihood = -194.08, time = 20.16s\n",
            "[BernoulliRBM] Iteration 46, pseudo-likelihood = -194.14, time = 20.94s\n",
            "[BernoulliRBM] Iteration 47, pseudo-likelihood = -194.01, time = 21.37s\n",
            "[BernoulliRBM] Iteration 48, pseudo-likelihood = -193.01, time = 20.00s\n",
            "[BernoulliRBM] Iteration 49, pseudo-likelihood = -192.93, time = 20.79s\n",
            "[BernoulliRBM] Iteration 50, pseudo-likelihood = -192.47, time = 21.07s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.98      0.99      0.99       995\n",
            "         1.0       0.99      0.99      0.99      1121\n",
            "         2.0       0.97      0.98      0.98      1015\n",
            "         3.0       0.97      0.97      0.97      1033\n",
            "         4.0       0.98      0.97      0.98       976\n",
            "         5.0       0.97      0.96      0.97       884\n",
            "         6.0       0.99      0.98      0.98       999\n",
            "         7.0       0.98      0.98      0.98      1034\n",
            "         8.0       0.96      0.96      0.96       923\n",
            "         9.0       0.96      0.97      0.96      1020\n",
            "\n",
            "    accuracy                           0.98     10000\n",
            "   macro avg       0.98      0.98      0.98     10000\n",
            "weighted avg       0.98      0.98      0.98     10000\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "v) Σε αυτό το σημείο θα αναλύσουμε τα αποτελέσματα που μας έδωσαν οι υλοποιήσεις των αλγορίθμων:\n",
        "\n",
        "Το μοντέλο της λογιστικής παλινδρόμησης χωρίς RBM\t δίνει accuracy 92%.\n",
        "Το μοντέλο της λογιστικής παλινδρόμησης με RBM δίνει accuracy 97%.\n",
        "Το μοντέλο της λογιστικής παλινδρόμησης με DBN από 2 RBM δίνει accuracy\t98%.\n",
        "\n",
        "Επίσης, σε κάθε μοντέλο καταγράφεται το precision, το recall και το f1-score. Σε κάθε περίπτωση οι τιμές αυτές είναι πάνω από 90%, το οποίο δείχνει  μεγάλη αξιοπιστία και ακρίβεια όσο αφορά τις προβλέψεις, καθώς και μεγάλη εγκυρότητα για το σύνολο δεδομένων  που χρησιμοποιήθηκε.\n",
        "\n",
        "Με βάση το αποτέλεσμα, τη κάλυτερη επίδοση σημειώνει το μοντέλο λογιστικής παλινδρόμησης με DBN από 2 RBM. Σίγουρα, η επίδραση που έχει η προσθήκη του/των RBM(s) στο συνολικό χρόνο εκτέλεσης δεν είναι αμελητέα, όμως η αύξηση της συνολικής ακρίβειας στην οποία οδηγούν σίγουρα έχει πολύ υψηλότερη βαρύτητα.\n",
        "\n"
      ],
      "metadata": {
        "id": "-qkxdrtBtOie"
      }
    }
  ]
}